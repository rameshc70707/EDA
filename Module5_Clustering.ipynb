{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4+u1d8MXpW3+/8FYTeDrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshc70707/EDA/blob/main/Module5_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_WJ24l2kcHd"
      },
      "outputs": [],
      "source": [
        "K-Means Clustering\n",
        "The K-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean µ of the samples in the cluster. The means are commonly called the cluster “centroids”; note that they are not, in general, points from X, although they live in the same space. The K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum of squared criterion:\n",
        "\n",
        "image.png\n",
        "\n",
        "Distance Measure\n",
        "image.png\n",
        "\n",
        "Importing the libraries\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "Importing the dataset\n",
        "\n",
        "[ ]\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/salemprakash/EDA/refs/heads/main/Data/Mall_Customers.csv')\n",
        "X = dataset.iloc[:, [3, 4]].values\n",
        "Using the elbow method to find the optimal number of clusters\n",
        "\n",
        "[ ]\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
        "    kmeans.fit(X)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n",
        "\n",
        "Training the K-Means model on the dataset\n",
        "\n",
        "[ ]\n",
        "kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "Visualising the clusters\n",
        "\n",
        "[ ]\n",
        "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
        "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
        "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
        "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
        "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
        "plt.title('Clusters of customers')\n",
        "plt.xlabel('Annual Income (k$)')\n",
        "plt.ylabel('Spending Score (1-100)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "Hierarchical Clustering\n",
        "Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example, all files and folders on the hard disk are organized in a hierarchy. There are two types of hierarchical clustering, Divisive and Agglomerative.\n",
        "\n",
        "Divisive method\n",
        "\n",
        "In this method we assign all of the observations to a single cluster and then partition the cluster to two least similar clusters. Finally, we proceed recursively on each cluster until there is one cluster for each observation.\n",
        "\n",
        "Agglomerative method In this method we assign each observation to its own cluster. Then, compute the similarity (e.g., distance) between each of the clusters and join the two most similar clusters. Finally, repeat steps 2 and 3 until there is only a single cluster left.\n",
        "\n",
        "Linkage or distance matrix\n",
        "Before any clustering is performed, it is required to determine the proximity matrix containing the distance between each point using a distance function. Then, the matrix is updated to display the distance between each cluster. The following three methods differ in how the distance between each cluster is measured.\n",
        "\n",
        "Single Linkage In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points. image.png\n",
        "\n",
        "Complete Linkage In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points. image.png\n",
        "\n",
        "Average Linkage In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other. Dendograms image.png\n",
        "\n",
        "Dendograms are tree diagrams frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. The clades are arranged according to how similar (or dissimilar) they are. Clades that are close to the same height are similar to each other; clades with different heights are dissimilar — the greater the difference in height, the more dissimilarity.\n",
        "\n",
        "An example involving the famous Iris data set is shown below. image.png\n",
        "\n",
        "\n",
        "[ ]\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "[ ]\n",
        "#Import the Mall Customer dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/salemprakash/EDA/refs/heads/main/Data/Mall_Customers.csv')\n",
        "\n",
        "\n",
        "[ ]\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.title(\"Annual income distribution\",fontsize=16)\n",
        "plt.xlabel (\"Annual income (k$)\",fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.hist(df['Annual Income (k$)'],color='orange',edgecolor='k')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "[ ]\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.title(\"Spending Score distribution\",fontsize=16)\n",
        "plt.xlabel (\"Spending Score (1-100)\",fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.hist(df['Spending Score (1-100)'],color='green',edgecolor='k')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "[ ]\n",
        "\n",
        "#Dendograms\n",
        "X = df.iloc[:,[3,4]].values\n",
        "\n",
        "[ ]\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean distances')\n",
        "#plt.grid(True)\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.show()\n",
        "\n",
        "Optimal number of clusters\n",
        "Often, the optimal number of clusters can be found from a Dendogram is a simple manner.\n",
        "\n",
        "Look for the longest stretch of vertical line which is not crossed by any extended horizontal lines (here extended means horizontal lines i.e. the cluster dividers are extended infinitely to both directions).\n",
        "Now take any point on that stretch of line and draw an imaginary horizontal line.\n",
        "Count how many vertical lines this imaginary lines crossed.\n",
        "That is likely to be the optimal number of clusters.\n",
        "\n",
        "[ ]\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.hlines(y=190,xmin=0,xmax=2000,lw=3,linestyles='--')\n",
        "plt.text(x=900,y=220,s='Horizontal line crossing 5 vertical lines',fontsize=20)\n",
        "#plt.grid(True)\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}